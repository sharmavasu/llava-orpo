{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50201c7f-c1c0-41c6-a5e8-5633752b81cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running final working test...\n",
      "üß™ Final test: 'What color is this image?'\n",
      "Step 1: Creating test image...\n",
      "‚úÖ Image created: (336, 336)\n",
      "Step 2: Processing image...\n",
      "‚ùå Error: name 'image_processor' is not defined\n",
      "\n",
      "üéâ FINAL RESULT: Failed: name 'image_processor' is not defined\n",
      "\n",
      "==================================================\n",
      "üß™ Final test: 'Describe what you see.'\n",
      "Step 1: Creating test image...\n",
      "‚úÖ Image created: (336, 336)\n",
      "Step 2: Processing image...\n",
      "‚ùå Error: name 'image_processor' is not defined\n",
      "\n",
      "üéâ SECOND TEST: Failed: name 'image_processor' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_37383/3477729672.py\", line 21, in final_working_test\n",
      "    image_tensor = process_images([image], image_processor, model.config)\n",
      "NameError: name 'image_processor' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_37383/3477729672.py\", line 21, in final_working_test\n",
      "    image_tensor = process_images([image], image_processor, model.config)\n",
      "NameError: name 'image_processor' is not defined\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Verifying installation...\")\n",
    "\n",
    "try:\n",
    "    # Test basic imports\n",
    "    import torch\n",
    "    print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "    print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    # Test LLaVA imports\n",
    "    from llava.model.builder import load_pretrained_model\n",
    "    from llava.mm_utils import get_model_name_from_path\n",
    "    print(\"‚úÖ LLaVA imports successful!\")\n",
    "    \n",
    "    # Check if flash_attn is available\n",
    "    try:\n",
    "        import flash_attn\n",
    "        print(\"‚úÖ Flash Attention available!\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è Flash Attention not available (but LLaVA should still work)\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Some components may not be properly installed.\")\n",
    "\n",
    "print(\"\\nüéâ Installation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "003ec34a-2ce3-4c12-af4f-ed34811fb60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.conversation import conv_templates\n",
    "from llava.mm_utils import tokenizer_image_token, process_images, IMAGE_TOKEN_INDEX\n",
    "from PIL import Image\n",
    "import torch\n",
    "import traceback # Moved import to top level for clarity, though it's only used in except block\n",
    "\n",
    "# Cell 10: LLaVA test with proper dtype handling\n",
    "def dtype_fixed_test(model, tokenizer, image_processor, question=\"What color is this image?\"):\n",
    "    \"\"\"\n",
    "    LLaVA test with proper dtype handling for quantized models\n",
    "    \n",
    "    Args:\n",
    "        model: The LLaVA model.\n",
    "        tokenizer: The tokenizer associated with the model.\n",
    "        image_processor: The image processor associated with the model.\n",
    "        question (str): The question to ask about the image.\n",
    "    \"\"\"\n",
    "    print(f\"üß™ Dtype-fixed test: '{question}'\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Create test image\n",
    "        print(\"Step 1: Creating test image...\")\n",
    "        image = Image.new('RGB', (336, 336), color=(255, 165, 0))  # Orange image\n",
    "        print(f\"‚úÖ Image created: {image.size}\")\n",
    "        \n",
    "        # Step 2: Process image with proper dtype\n",
    "        print(\"Step 2: Processing image...\")\n",
    "        # Ensure model.config is accessible; it's common for Hugging Face models\n",
    "        if not hasattr(model, 'config'):\n",
    "            print(\"‚ùå Error: Model does not have a 'config' attribute.\")\n",
    "            return \"Failed: Model does not have a 'config' attribute.\"\n",
    "            \n",
    "        image_tensor = process_images([image], image_processor, model.config)\n",
    "        \n",
    "        if image_tensor is None or len(image_tensor) == 0:\n",
    "            return \"‚ùå Image processing failed\"\n",
    "        \n",
    "        image_tensor = image_tensor[0]\n",
    "        print(f\"‚úÖ Image processed: {image_tensor.shape}\")\n",
    "        \n",
    "        # Step 3: Setup conversation\n",
    "        print(\"Step 3: Setting up conversation...\")\n",
    "        # Ensure the conversation template key is valid\n",
    "        if \"mistral_instruct\" not in conv_templates:\n",
    "            # Fallback or error if specific template is missing\n",
    "            # For LLaVA, common templates are 'llava_v1', 'vicuna_v1', 'llava_llama_2' etc.\n",
    "            # Using a generic or common one if \"mistral_instruct\" is not found.\n",
    "            # This part might need adjustment based on available templates.\n",
    "            # For now, let's assume 'llava_v1' is a more generic choice if 'mistral_instruct' fails.\n",
    "            # Or, the user needs to ensure \"mistral_instruct\" is correct.\n",
    "            # Forcing a known key for now to prevent crashes if \"mistral_instruct\" is not default for llava.\n",
    "            # If \"mistral_instruct\" is specific to their setup, they can keep it.\n",
    "            # A common LLaVA template is often 'llava_v1' or 'vicuna_v1'.\n",
    "            # Let's assume 'llava_v1' for robustness if 'mistral_instruct' is not present.\n",
    "            conv_template_key = \"llava_v1\" \n",
    "            if \"mistral_instruct\" in conv_templates:\n",
    "                conv_template_key = \"mistral_instruct\"\n",
    "            elif not conv_templates: # if conv_templates is empty\n",
    "                 print(\"‚ùå Error: conv_templates is empty.\")\n",
    "                 return \"Failed: conv_templates is empty.\"\n",
    "            elif conv_template_key not in conv_templates: # if llava_v1 is also not found\n",
    "                conv_template_key = list(conv_templates.keys())[0] # take the first available\n",
    "            \n",
    "            print(f\"‚ö†Ô∏è Warning: 'mistral_instruct' not found. Using '{conv_template_key}'.\")\n",
    "            conv = conv_templates[conv_template_key].copy()\n",
    "        else:\n",
    "            conv = conv_templates[\"mistral_instruct\"].copy()\n",
    "            \n",
    "        conv.append_message(conv.roles[0], question)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "        print(f\"‚úÖ Conversation ready\")\n",
    "        \n",
    "        # Step 4: Tokenize\n",
    "        print(\"Step 4: Tokenizing...\")\n",
    "        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "        if input_ids is None:\n",
    "            return \"‚ùå Tokenization failed\"\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "        print(f\"‚úÖ Tokenized: {input_ids.shape}\")\n",
    "        \n",
    "        # Step 5: Move to device and fix dtype\n",
    "        print(\"Step 5: Moving to device and fixing dtypes...\")\n",
    "        device = next(model.parameters()).device\n",
    "        model_dtype = next(model.parameters()).dtype\n",
    "        \n",
    "        print(f\"Model device: {device}\")\n",
    "        print(f\"Model dtype: {model_dtype}\")\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        image_tensor = image_tensor.to(device, dtype=model_dtype)\n",
    "        \n",
    "        image_sizes = [image.size]\n",
    "        \n",
    "        print(f\"‚úÖ Input IDs: {input_ids.dtype} on {input_ids.device}\")\n",
    "        print(f\"‚úÖ Image tensor: {image_tensor.dtype} on {image_tensor.device}\")\n",
    "        print(f\"‚úÖ Image sizes: {image_sizes}\")\n",
    "        \n",
    "        # Step 6: Generate with proper attention mask\n",
    "        print(\"Step 6: Generating...\")\n",
    "        with torch.inference_mode():\n",
    "            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "            \n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=image_tensor.unsqueeze(0), # model expects batch, so unsqueeze\n",
    "                image_sizes=image_sizes,\n",
    "                max_new_tokens=100,\n",
    "                use_cache=True,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        if output_ids is None:\n",
    "            return \"‚ùå Generation failed\"\n",
    "        \n",
    "        print(f\"‚úÖ Generated: {output_ids.shape}\")\n",
    "        \n",
    "        # Step 7: Decode\n",
    "        print(\"Step 7: Decoding...\")\n",
    "        # Ensure output_ids are on CPU for decoding if tokenizer expects that\n",
    "        outputs = tokenizer.batch_decode(output_ids.cpu(), skip_special_tokens=True)\n",
    "        \n",
    "        if not outputs:\n",
    "            return \"‚ùå Decoding failed\"\n",
    "        \n",
    "        response = outputs[0].strip()\n",
    "        \n",
    "        # Clean response: remove the prompt part if it's echoed\n",
    "        # This specific cleaning logic might need adjustment based on the model's output format\n",
    "        # A common way LLaVA echoes is \"ASSISTANT: <response>\" or just \"<response>\"\n",
    "        # The original logic for removing question was:\n",
    "        # if question in response:\n",
    "        #    response = response.split(question)[-1].strip()\n",
    "        # This might be too aggressive if the question is naturally part of the answer.\n",
    "        # A more robust way is to find the end of the prompt in the generated output.\n",
    "        # The prompt includes the question.\n",
    "        # `outputs[0]` contains the full sequence including prompt and generated part.\n",
    "        # We need to decode `input_ids` to get the prompt text sent to the model\n",
    "        # prompt_text_decoded = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        # if response.startswith(prompt_text_decoded):\n",
    "        #    response = response[len(prompt_text_decoded):].strip()\n",
    "            \n",
    "        # The provided cleaning looks for specific prefixes, which is fine.\n",
    "        # Also, the prompt itself sometimes appears as a prefix in the output.\n",
    "        # Let's try to remove the full prompt if it is repeated.\n",
    "        clean_prompt_for_checking = prompt.replace(IMAGE_TOKEN_INDEX, '').replace('<image>', '').strip()\n",
    "\n",
    "        # Check if the response starts with the prompt (common behavior)\n",
    "        # The prompt prepared `conv.get_prompt()` includes roles, e.g., \"USER: ... ASSISTANT:\"\n",
    "        # Often, the model output only contains the text after \"ASSISTANT:\"\n",
    "        # The original cleaning logic:\n",
    "        # if question in response:\n",
    "        #     response = response.split(question)[-1].strip()\n",
    "        # This is okay, but checking for standard role markers might be more general.\n",
    "\n",
    "        # The current prefix stripping loop is a good general approach:\n",
    "        cleaned_response = response\n",
    "        # If prompt ends with assistant's role, and model output starts with it, it can be stripped\n",
    "        assistant_role_marker = conv.roles[1] # e.g., \"ASSISTANT:\"\n",
    "        if cleaned_response.startswith(assistant_role_marker):\n",
    "            cleaned_response = cleaned_response[len(assistant_role_marker):].strip()\n",
    "        \n",
    "        # Then the custom prefixes:\n",
    "        for prefix in [\"[/INST]\", \"Assistant:\", \"ASSISTANT:\", \"A:\", \"Answer:\"]: # ensure consistency with actual output\n",
    "            if cleaned_response.startswith(prefix):\n",
    "                cleaned_response = cleaned_response[len(prefix):].strip()\n",
    "                break\n",
    "        \n",
    "        # If the question itself is repeated at the start of the cleaned response (after role markers)\n",
    "        if cleaned_response.lower().startswith(question.lower()):\n",
    "             temp_response = cleaned_response[len(question):].strip()\n",
    "             # check if the removal makes sense (e.g. starts with a colon or similar)\n",
    "             if temp_response.startswith(':') or temp_response.startswith('-') or len(temp_response) < len(cleaned_response) * 0.8:\n",
    "                 cleaned_response = temp_response.lstrip(':').strip()\n",
    "\n",
    "\n",
    "        print(f\"‚úÖ SUCCESS! Response: {cleaned_response}\")\n",
    "        return cleaned_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return f\"Failed: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ac84f-b75b-4c44-9491-ee855ca5a275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
