{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac55b91-a7eb-44b4-a3ff-48d45ab16ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Updated System Information:\n",
      "Python version: 3.10.17 (main, Apr 25 2025, 17:54:28) [GCC 11.4.0]\n",
      "Current working directory: /home/vasu/Documents/experiments/sereact\n",
      "✅ GPU detected:\n",
      "Mon May 26 14:11:50 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...    On  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   48C    P8               9W /  60W |     11MiB /  6144MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2257      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "🔥 PyTorch GPU Info:\n",
      "✅ PyTorch version: 2.1.2+cu121\n",
      "✅ CUDA available: True\n",
      "✅ CUDA version: 12.1\n",
      "✅ GPU device: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "✅ GPU memory: 5.8 GB\n",
      "\n",
      "💾 System Memory:\n",
      "✅ Total RAM: 30.8 GB\n",
      "\n",
      "==================================================\n",
      "🎉 GPU setup is ready for LLaVA!\n",
      "👉 You can now proceed with the installation cells\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"🔍 Updated System Information:\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check GPU availability with detailed info\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ GPU detected:\")\n",
    "        print(result.stdout)\n",
    "\n",
    "        # Also check with PyTorch\n",
    "        try:\n",
    "            import torch\n",
    "            print(f\"\\n🔥 PyTorch GPU Info:\")\n",
    "            print(f\"✅ PyTorch version: {torch.__version__}\")\n",
    "            print(f\"✅ CUDA available: {torch.cuda.is_available()}\")\n",
    "            if torch.cuda.is_available():\n",
    "                print(f\"✅ CUDA version: {torch.version.cuda}\")\n",
    "                print(f\"✅ GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "                print(f\"✅ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "            else:\n",
    "                print(\"❌ CUDA not available in PyTorch\")\n",
    "        except ImportError:\n",
    "            print(\"📦 PyTorch not yet installed - will be installed with LLaVA\")\n",
    "\n",
    "    else:\n",
    "        print(\"❌ No GPU detected\")\n",
    "        print(\"Please enable GPU runtime: Runtime → Change runtime type → T4 GPU\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ nvidia-smi not found - no GPU available\")\n",
    "    print(\"🔧 Please enable GPU runtime: Runtime → Change runtime type → T4 GPU\")\n",
    "\n",
    "# Memory info\n",
    "print(f\"\\n💾 System Memory:\")\n",
    "try:\n",
    "    with open('/proc/meminfo', 'r') as f:\n",
    "        meminfo = f.read()\n",
    "        for line in meminfo.split('\\n'):\n",
    "            if 'MemTotal' in line:\n",
    "                total_mem = int(line.split()[1]) / 1024 / 1024  # Convert to GB\n",
    "                print(f\"✅ Total RAM: {total_mem:.1f} GB\")\n",
    "                break\n",
    "except:\n",
    "    print(\"❓ Could not determine system memory\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if 'nvidia-smi' in str(subprocess.run(['which', 'nvidia-smi'], capture_output=True, text=True).stdout):\n",
    "    print(\"🎉 GPU setup is ready for LLaVA!\")\n",
    "    print(\"👉 You can now proceed with the installation cells\")\n",
    "else:\n",
    "    print(\"⚠️  No GPU detected - LLaVA will be very slow on CPU\")\n",
    "    print(\"🔧 Strongly recommend enabling GPU runtime first\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dfba9ef-044e-47fd-a388-730f7a5a9179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50201c7f-c1c0-41c6-a5e8-5633752b81cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Verifying installation...\n",
      "✅ PyTorch version: 2.1.2+cu121\n",
      "✅ CUDA available: True\n",
      "❌ Import error: No module named 'llava'\n",
      "Some components may not be properly installed.\n",
      "\n",
      "🎉 Installation complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"🔍 Verifying installation...\")\n",
    "\n",
    "try:\n",
    "    # Test basic imports\n",
    "    import torch\n",
    "    print(f\"✅ PyTorch version: {torch.__version__}\")\n",
    "    print(f\"✅ CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    # Test LLaVA imports\n",
    "    from LLaVA.llava.model.builder import load_pretrained_model\n",
    "    from LLaVA.llava.mm_utils import get_model_name_from_path\n",
    "    print(\"✅ LLaVA imports successful!\")\n",
    "    \n",
    "    # Check if flash_attn is available\n",
    "    try:\n",
    "        import flash_attn\n",
    "        print(\"✅ Flash Attention available!\")\n",
    "    except ImportError:\n",
    "        print(\"⚠️ Flash Attention not available (but LLaVA should still work)\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"Some components may not be properly installed.\")\n",
    "\n",
    "print(\"\\n🎉 Installation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003ec34a-2ce3-4c12-af4f-ed34811fb60b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llava'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m conv_templates\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmm_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tokenizer_image_token, process_images, IMAGE_TOKEN_INDEX\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llava'"
     ]
    }
   ],
   "source": [
    "from llava.conversation import conv_templates\n",
    "from llava.mm_utils import tokenizer_image_token, process_images, IMAGE_TOKEN_INDEX\n",
    "from PIL import Image\n",
    "import torch\n",
    "import traceback # Moved import to top level for clarity, though it's only used in except block\n",
    "\n",
    "# Cell 10: LLaVA test with proper dtype handling\n",
    "def dtype_fixed_test(model, tokenizer, image_processor, question=\"What color is this image?\"):\n",
    "    \"\"\"\n",
    "    LLaVA test with proper dtype handling for quantized models\n",
    "    \n",
    "    Args:\n",
    "        model: The LLaVA model.\n",
    "        tokenizer: The tokenizer associated with the model.\n",
    "        image_processor: The image processor associated with the model.\n",
    "        question (str): The question to ask about the image.\n",
    "    \"\"\"\n",
    "    print(f\"🧪 Dtype-fixed test: '{question}'\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Create test image\n",
    "        print(\"Step 1: Creating test image...\")\n",
    "        image = Image.new('RGB', (336, 336), color=(255, 165, 0))  # Orange image\n",
    "        print(f\"✅ Image created: {image.size}\")\n",
    "        \n",
    "        # Step 2: Process image with proper dtype\n",
    "        print(\"Step 2: Processing image...\")\n",
    "        # Ensure model.config is accessible; it's common for Hugging Face models\n",
    "        if not hasattr(model, 'config'):\n",
    "            print(\"❌ Error: Model does not have a 'config' attribute.\")\n",
    "            return \"Failed: Model does not have a 'config' attribute.\"\n",
    "            \n",
    "        image_tensor = process_images([image], image_processor, model.config)\n",
    "        \n",
    "        if image_tensor is None or len(image_tensor) == 0:\n",
    "            return \"❌ Image processing failed\"\n",
    "        \n",
    "        image_tensor = image_tensor[0]\n",
    "        print(f\"✅ Image processed: {image_tensor.shape}\")\n",
    "        \n",
    "        # Step 3: Setup conversation\n",
    "        print(\"Step 3: Setting up conversation...\")\n",
    "        # Ensure the conversation template key is valid\n",
    "        if \"mistral_instruct\" not in conv_templates:\n",
    "            # Fallback or error if specific template is missing\n",
    "            # For LLaVA, common templates are 'llava_v1', 'vicuna_v1', 'llava_llama_2' etc.\n",
    "            # Using a generic or common one if \"mistral_instruct\" is not found.\n",
    "            # This part might need adjustment based on available templates.\n",
    "            # For now, let's assume 'llava_v1' is a more generic choice if 'mistral_instruct' fails.\n",
    "            # Or, the user needs to ensure \"mistral_instruct\" is correct.\n",
    "            # Forcing a known key for now to prevent crashes if \"mistral_instruct\" is not default for llava.\n",
    "            # If \"mistral_instruct\" is specific to their setup, they can keep it.\n",
    "            # A common LLaVA template is often 'llava_v1' or 'vicuna_v1'.\n",
    "            # Let's assume 'llava_v1' for robustness if 'mistral_instruct' is not present.\n",
    "            conv_template_key = \"llava_v1\" \n",
    "            if \"mistral_instruct\" in conv_templates:\n",
    "                conv_template_key = \"mistral_instruct\"\n",
    "            elif not conv_templates: # if conv_templates is empty\n",
    "                 print(\"❌ Error: conv_templates is empty.\")\n",
    "                 return \"Failed: conv_templates is empty.\"\n",
    "            elif conv_template_key not in conv_templates: # if llava_v1 is also not found\n",
    "                conv_template_key = list(conv_templates.keys())[0] # take the first available\n",
    "            \n",
    "            print(f\"⚠️ Warning: 'mistral_instruct' not found. Using '{conv_template_key}'.\")\n",
    "            conv = conv_templates[conv_template_key].copy()\n",
    "        else:\n",
    "            conv = conv_templates[\"mistral_instruct\"].copy()\n",
    "            \n",
    "        conv.append_message(conv.roles[0], question)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "        print(f\"✅ Conversation ready\")\n",
    "        \n",
    "        # Step 4: Tokenize\n",
    "        print(\"Step 4: Tokenizing...\")\n",
    "        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "        if input_ids is None:\n",
    "            return \"❌ Tokenization failed\"\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "        print(f\"✅ Tokenized: {input_ids.shape}\")\n",
    "        \n",
    "        # Step 5: Move to device and fix dtype\n",
    "        print(\"Step 5: Moving to device and fixing dtypes...\")\n",
    "        device = next(model.parameters()).device\n",
    "        model_dtype = next(model.parameters()).dtype\n",
    "        \n",
    "        print(f\"Model device: {device}\")\n",
    "        print(f\"Model dtype: {model_dtype}\")\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        image_tensor = image_tensor.to(device, dtype=model_dtype)\n",
    "        \n",
    "        image_sizes = [image.size]\n",
    "        \n",
    "        print(f\"✅ Input IDs: {input_ids.dtype} on {input_ids.device}\")\n",
    "        print(f\"✅ Image tensor: {image_tensor.dtype} on {image_tensor.device}\")\n",
    "        print(f\"✅ Image sizes: {image_sizes}\")\n",
    "        \n",
    "        # Step 6: Generate with proper attention mask\n",
    "        print(\"Step 6: Generating...\")\n",
    "        with torch.inference_mode():\n",
    "            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "            \n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=image_tensor.unsqueeze(0), # model expects batch, so unsqueeze\n",
    "                image_sizes=image_sizes,\n",
    "                max_new_tokens=100,\n",
    "                use_cache=True,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        if output_ids is None:\n",
    "            return \"❌ Generation failed\"\n",
    "        \n",
    "        print(f\"✅ Generated: {output_ids.shape}\")\n",
    "        \n",
    "        # Step 7: Decode\n",
    "        print(\"Step 7: Decoding...\")\n",
    "        # Ensure output_ids are on CPU for decoding if tokenizer expects that\n",
    "        outputs = tokenizer.batch_decode(output_ids.cpu(), skip_special_tokens=True)\n",
    "        \n",
    "        if not outputs:\n",
    "            return \"❌ Decoding failed\"\n",
    "        \n",
    "        response = outputs[0].strip()\n",
    "        \n",
    "        # Clean response: remove the prompt part if it's echoed\n",
    "        # This specific cleaning logic might need adjustment based on the model's output format\n",
    "        # A common way LLaVA echoes is \"ASSISTANT: <response>\" or just \"<response>\"\n",
    "        # The original logic for removing question was:\n",
    "        # if question in response:\n",
    "        #    response = response.split(question)[-1].strip()\n",
    "        # This might be too aggressive if the question is naturally part of the answer.\n",
    "        # A more robust way is to find the end of the prompt in the generated output.\n",
    "        # The prompt includes the question.\n",
    "        # `outputs[0]` contains the full sequence including prompt and generated part.\n",
    "        # We need to decode `input_ids` to get the prompt text sent to the model\n",
    "        # prompt_text_decoded = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        # if response.startswith(prompt_text_decoded):\n",
    "        #    response = response[len(prompt_text_decoded):].strip()\n",
    "            \n",
    "        # The provided cleaning looks for specific prefixes, which is fine.\n",
    "        # Also, the prompt itself sometimes appears as a prefix in the output.\n",
    "        # Let's try to remove the full prompt if it is repeated.\n",
    "        clean_prompt_for_checking = prompt.replace(IMAGE_TOKEN_INDEX, '').replace('<image>', '').strip()\n",
    "\n",
    "        # Check if the response starts with the prompt (common behavior)\n",
    "        # The prompt prepared `conv.get_prompt()` includes roles, e.g., \"USER: ... ASSISTANT:\"\n",
    "        # Often, the model output only contains the text after \"ASSISTANT:\"\n",
    "        # The original cleaning logic:\n",
    "        # if question in response:\n",
    "        #     response = response.split(question)[-1].strip()\n",
    "        # This is okay, but checking for standard role markers might be more general.\n",
    "\n",
    "        # The current prefix stripping loop is a good general approach:\n",
    "        cleaned_response = response\n",
    "        # If prompt ends with assistant's role, and model output starts with it, it can be stripped\n",
    "        assistant_role_marker = conv.roles[1] # e.g., \"ASSISTANT:\"\n",
    "        if cleaned_response.startswith(assistant_role_marker):\n",
    "            cleaned_response = cleaned_response[len(assistant_role_marker):].strip()\n",
    "        \n",
    "        # Then the custom prefixes:\n",
    "        for prefix in [\"[/INST]\", \"Assistant:\", \"ASSISTANT:\", \"A:\", \"Answer:\"]: # ensure consistency with actual output\n",
    "            if cleaned_response.startswith(prefix):\n",
    "                cleaned_response = cleaned_response[len(prefix):].strip()\n",
    "                break\n",
    "        \n",
    "        # If the question itself is repeated at the start of the cleaned response (after role markers)\n",
    "        if cleaned_response.lower().startswith(question.lower()):\n",
    "             temp_response = cleaned_response[len(question):].strip()\n",
    "             # check if the removal makes sense (e.g. starts with a colon or similar)\n",
    "             if temp_response.startswith(':') or temp_response.startswith('-') or len(temp_response) < len(cleaned_response) * 0.8:\n",
    "                 cleaned_response = temp_response.lstrip(':').strip()\n",
    "\n",
    "\n",
    "        print(f\"✅ SUCCESS! Response: {cleaned_response}\")\n",
    "        return cleaned_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return f\"Failed: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ac84f-b75b-4c44-9491-ee855ca5a275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
